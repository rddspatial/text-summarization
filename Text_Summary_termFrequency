#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jul 30 15:02:56 2019

@author: Rahul
"""

import pandas as pd
import nltk
import re
import numpy as np
import math
from heapq import nlargest
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import csv

src = '/Users/Rahul/Documents/Grices maxim/data/Tweet_convo manual data collection_1 to 200_Test_2.csv'

trgt='/Users/Rahul/Documents/Research/Kiran/summary/word_freq_Test.csv'
df = pd.read_csv(src, sep=',', encoding='ISO-8859-1')

   
 
row_list=list() 

#print(df.columns)
#print (df['article_text'].head())

sentences = []
temp_sentences = []
flatten_sentences = []
flat_sent = ''

#for s in df['article_text']:
#    temp_sentences = sent_tokenize(s)
#    sentences.append(temp_sentences)
#    
#for index, s in enumerate(sentences):
##    print ('Article:',index, s,'\n')
#    for entry in s:
#        flat_sent = flat_sent + entry
    
#print (flat_sent)
        
test=''

thread_len=201 #number of the threads
for i in range(thread_len):
    sentences=list()
    test=''
    sentences=list()
    summary_list=list()
    print (i)
    if (i>0):
        for conv_id, text in zip(df['Conv ID'], df['Content']):
           # print ('c: ',conv_id)
            if (conv_id==i):
               # print ('text: ',text)
                text=text.replace('\x92',' ')
                text=text.replace('\x93',' ')
                text=text.replace('\x94',' ')
                text=text.replace('\x85',' ')
                sentences.append(text)
        if sentences[-1] != '.':
            sentences.append('.')        
        test = ' '.join(sentences)            
        stopwordlist = stopwords.words('english')
        word_freq = {}
        for sent in sent_tokenize(test):
            sent = re.sub(r'[^\w\s]','',sent)
            for word in word_tokenize(sent):
                if word not in stopwordlist:
                    if word not in word_freq:
                        word_freq[word] = 1
                    else:
                        word_freq[word] += 1
                    
        print (word_freq)
        num_sent=(sent_tokenize(test))
        cutoff=0.3
        threshold=int(math.ceil(len(num_sent)*cutoff))
        print ('THRESHOLD: ',threshold, ', # of sentences: ',len(num_sent))
        
        maximum_freq = max (word_freq.values())
        max_word = max (word_freq, key = lambda key: word_freq[key])
        
        #print ('maximum frequent word is: ',max_word,' with frequency count =', maximum_freq)
        
        top_k = nlargest (2, word_freq, key = word_freq.get)
        max_word = str(top_k[0])
        max_word_value = word_freq[max_word]
        #print (max_word, ': ', word_freq[max_word])
        #print (word_freq[max_word])
        
        for entry in word_freq:
        #    print (entry, ': ', word_freq[entry], ': ',  max_word, ': ', max_word_value)
            word_freq[entry] = (word_freq[entry]/max_word_value)
            
        print (word_freq)
        
        sent_freq = {}
        
        for sent in sent_tokenize(test):
            sent_score = 0
            for word in word_tokenize (sent):
                if word in word_freq:
                    sent_score = sent_score + word_freq[word]
        #    print (sent_score)
            sent_freq[sent] = sent_score
            
        print ('Input text: ', test)
        
        
        for entry, value in sent_freq.items():
            print (entry,': ',value)
        top_k = nlargest (threshold, sent_freq, key = sent_freq.get)
        
        print ('Summary: ')
        for entry in top_k:
            print (entry)
            summary_list.append(entry)
            
        summary=' '.join(summary_list)
      
        row=[i,summary]
        row_list.append(row)
            
header = ['Conv_ID','Summary']
with open(trgt, 'wt') as f:
    csv_writer = csv.writer(f)
    csv_writer.writerow(header) 
     
    for record in row_list:
        print(record)
        csv_writer.writerow(record)
                